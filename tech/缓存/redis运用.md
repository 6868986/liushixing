分布式锁：

不同JVM进程对共享资源的互斥访问（互斥、高可用、可重入）

setnx + del （lua脚本 => 防止误删其他锁）

释放锁的逻辑突然挂掉 => 设置过期时间 => 共享资源最终会释放

锁的优雅续期 => Java -> Redission (其中的分布式锁自带自动续期机制）=> WatchDog（每隔10s - 通过lua脚本执行续期操作 - EX 30s）

可重入锁 => 为锁关联一个可重入计数器&持有锁的线程

集群情况下分布式锁的可靠性（存在的问题）：RedLock算法

 

底层数据结构：

ZipList：一块连续的内存空间，元素连续存储，无冗余空隙 => 提高存储效率

bytes、tail_offset（快速定位最后一个元素）、length（16bit => length<2^16 - 1,否则遍历）、entries（prevlen）、end（0xFF 255）

QuickList：LinkedList缺点（附加空间+节点单独分配导致内存碎片化）

ziplist + prev + next = quicklistNode（LinkedList按段切分，每段使用ziplist紧凑存储）

插入：头部/尾部、任意位置

SkipList：链表+多级索引（空间换时间） <= Zset中元素过多/元素成员为较长的字符串

Redis中的跳跃表：zskiplistNode（level、backward、score、obj） + zskiplist（header、tail、level、length）

 

String：SDS

List：LinkedList+ZipList => QuickList

Hash：ZipList、HashTable

Set：intset（整数集合）、HashTable

Zset：ZipList、SkipList、HashTable

 

 

Redis Cluster

特点：多主多从，去中心化；支持节点的动态扩容，不收单机容量配置的影响；

节点之间相互通信，相互选举，不再依赖哨兵；不支持处理多个key

节点只能使用0号数据库

数据分片：哈希槽slot

cluster keysolt ： CRC16算法然后对16384进行取模

在redis节点发送心跳包时需要把所有的槽放到这个心跳包里，以便让节点知道当前集群信息，16384=16k，在发送心跳包时使用`char`进行bitmap压缩后是2k（`2 * 8 (8 bit) * 1024(1k) = 16K`），也就是说使用2k的空间创建了16k的槽数。

虽然使用CRC16算法最多可以分配65535（2^16-1）个槽位，65535=65k，压缩后就是8k（`8 * 8 (8 bit) * 1024(1k) =65K`），也就是说需要需要8k的心跳包，作者认为这样做不太值得；并且一般情况下一个redis集群不会有超过1000个master节点，所以16k的槽位是个比较合适的选择。

重新分片：将任意数量的已指派给某个节点的槽指派给一个新的节点（目标节点），该操作可以在线执行，由集群管理软件redis-trib负责。

如何保证HA（高可用性）：故障转移&故障检测

1. 故障转移：在该下线的主节点的所有从节点中选举一个，执行slaveof no one命令，成为新的主节点，然后撤销槽指派，全部指派给自己，主节点向集群广播一条PONG消息告知，新主节点开始接受和处理自己负责的槽有关的命令请求。

选举机制：

2. 故障检测：集群中每个节点都会定时向其他节点发送PING命令，未在规定时间内回复PONG则将其标记为PFAIL；当半数以上处理槽的主节点都将某主节点x标记为PFAIL，则x被标记为FAIL，某主节点y会向集群广播这条消息。

 

 

分布式事务：如何实现（模型）

 

 

 

 

一致性哈希算法：为了防止集群中某些节点挂掉/新加入某些节点 而导致rehash、重新分片、数据迁移

容错性 && 扩展性

负载均衡：每个节点数据相同 => 加权轮询算法

分布式集群中，每个节点存储不同的数据Redis Cluster => 必须请求某个指定的节点 => 哈希算法 => 不具有容错性&可扩展性

一致性哈希 => 减少数据迁移量 but节点分布不均匀会导致达不到负载均衡

虚拟节点 => 提高系统的负载均衡度 => 先找到虚拟节点，再根据虚拟节点去找真实节点，

虚拟节点映射到哈希环 && 映射到实际节点

权重更高的真实节点可分配更多虚拟节点





### Redis HA

- **IO模型**

  ***阻塞非阻塞***指的是在获取结果上是否会阻塞等待结果完成，这段时间能不能做别的事
  ***同步异步***关注的是方式，是主动获取还是被动等回调

  epoll：eventpoll  事件驱动模型

  1. epoll_create函数，创建eventpoll结构体，包括一个注册fd并监听的红黑树  &&  就绪链表
  2. epoll_ctl函数，将fd注册到红黑树，并注册上对应事件的回调函数
  3. epoll_wait函数，等待文件事件的发生
  4. 红黑树将监听到的事件添加一份到就绪队列
  5. 内核唤醒用户线程，并将就绪链表中的fd拷贝到用户空间
  6. 用户 直接取出eventpoll中对应的回调函数并执行

  **epoll优点**
  ●EPOLL支持的最大文件描述符上限是整个系统最大可打开的文件数目, 1G内存理论上最大创建10万个文件描述符
  ●每个文件描述符上都有一个callback函数，当socket有事件发生时会回调这个函数将该fd的引用添加到就绪列表中，                  select和poll并不会明确指出是哪些文件描述符就绪，而epoll会。                                                                                                                  造成的区别就是，系统调用返回后，调用select和poll的程序需要遍历监听的整个文件描述符找到是谁处于就绪，而epoll则直接处理即可
  ●select、poll采用轮询的方式来检查文件描述符是否处于就绪态，而epoll采用回调机制。造成的结果就是，随着fd的增加，select和poll的效率会线性降低，而epoll不会受到太大影响，除非活跃的socket很多

- **哨兵机制**

  - 自动故障检测

    超过参数x个哨兵都将某主节点标记为主观下线状态

  - 自动故障转移

    1. 哨兵leader的选举
       - 票数 > n / 2 + 1（半数以上）
       - 票数 > 参数x
    2. 选举新的master节点

  - 通知客户端、为客户端提供主节点的地址（初始化 && 故障转移后）

- Cluster

  - 请求重定向

    ​	集群存在伸缩 => 已知的槽和集群节点的映射关系发生改变

    1. MOVED

       key命令 => 节点计算出对应的槽和节点 => 对应节点已经变化 => 发送MOVED重定向 => 根据MOVED去新节点请求

    2. ASK

       发生在集群伸缩期间

  - Goosip协议

    1. MEET	加入一个节点到集群
    2. PING     两个节点的地址、槽指派、最近一次通信时间、***状态信息***
    3. PONG    回复PING
    4. Fail         下线

  - 故障检测

    节点每隔1s随机选出5个古董节点，向它们当中最久没联系（没收到PONG）的节点发送PING，没有在指定时间内回复，则标记为PFAIL，然后广播该消息，超过半数节点标记为PFAIL，则节点变为FAIL

  - 故障转移

    master节点挂掉后，currentEpoch，子节点开始发送PING请求投票，收到消息的主节点会先进行合法性判断，而后投票，子节点收集的票数超过半数后成为新的master，PONG广播通知其他节点

  - 

- 读写分离、增加从库

- 热点Key探测 => redis与客户端之间加一层代理

### 
